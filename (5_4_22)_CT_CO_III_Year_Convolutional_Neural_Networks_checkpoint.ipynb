
Part 1: Convolutional Neural Network
Importing packages
import numpy as np
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from keras import backend as K
from keras.preprocessing import image
from keras.applications.mobilenet import MobileNet
from keras.applications.vgg16 import preprocess_input, decode_predictions
from keras.models import Model
import timeit

import warnings
warnings.filterwarnings('ignore')
from tensorflow.keras.utils import to_categorical
Preparing Dataset
batch_size = 64
num_classes = 10
epochs = 50

# input image dimensions
img_rows, img_cols = 28, 28

# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()

if K.image_data_format() == 'channels_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (1, img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# convert class vectors to binary class matrices
#y_train = keras.utils.to_categorical(y_train, num_classes)
y_train = to_categorical(y_train, num_classes)
y_test =to_categorical(y_test, num_classes)
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Building a Model
model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(16, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
Model: "sequential_2"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 13, 13, 32)       0         
 2D)                                                             
                                                                 
 conv2d_5 (Conv2D)           (None, 11, 11, 16)        4624      
                                                                 
 max_pooling2d_5 (MaxPooling  (None, 5, 5, 16)         0         
 2D)                                                             
                                                                 
 dropout_4 (Dropout)         (None, 5, 5, 16)          0         
                                                                 
 flatten_2 (Flatten)         (None, 400)               0         
                                                                 
 dense_4 (Dense)             (None, 100)               40100     
                                                                 
 dropout_5 (Dropout)         (None, 100)               0         
                                                                 
 dense_5 (Dense)             (None, 10)                1010      
                                                                 
=================================================================
Total params: 46,054
Trainable params: 46,054
Non-trainable params: 0
_________________________________________________________________
Model Training
from tensorflow import keras
from tensorflow.keras import layers

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer="adam",
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
Epoch 1/50
938/938 [==============================] - 7s 7ms/step - loss: 0.4081 - accuracy: 0.8703 - val_loss: 0.0829 - val_accuracy: 0.9731
Epoch 2/50
938/938 [==============================] - 7s 7ms/step - loss: 0.1499 - accuracy: 0.9544 - val_loss: 0.0576 - val_accuracy: 0.9810
Epoch 3/50
938/938 [==============================] - 6s 7ms/step - loss: 0.1184 - accuracy: 0.9632 - val_loss: 0.0450 - val_accuracy: 0.9840
Epoch 4/50
938/938 [==============================] - 6s 7ms/step - loss: 0.1021 - accuracy: 0.9698 - val_loss: 0.0365 - val_accuracy: 0.9879
Epoch 5/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0893 - accuracy: 0.9726 - val_loss: 0.0338 - val_accuracy: 0.9887
Epoch 6/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0810 - accuracy: 0.9760 - val_loss: 0.0312 - val_accuracy: 0.9906
Epoch 7/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0768 - accuracy: 0.9765 - val_loss: 0.0318 - val_accuracy: 0.9894
Epoch 8/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0722 - accuracy: 0.9779 - val_loss: 0.0305 - val_accuracy: 0.9897
Epoch 9/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0669 - accuracy: 0.9801 - val_loss: 0.0278 - val_accuracy: 0.9909
Epoch 10/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0646 - accuracy: 0.9801 - val_loss: 0.0254 - val_accuracy: 0.9917
Epoch 11/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0610 - accuracy: 0.9817 - val_loss: 0.0262 - val_accuracy: 0.9918
Epoch 12/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.0276 - val_accuracy: 0.9916
Epoch 13/50
938/938 [==============================] - 7s 7ms/step - loss: 0.0594 - accuracy: 0.9816 - val_loss: 0.0257 - val_accuracy: 0.9925
Epoch 14/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0544 - accuracy: 0.9835 - val_loss: 0.0247 - val_accuracy: 0.9915
Epoch 15/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0559 - accuracy: 0.9822 - val_loss: 0.0235 - val_accuracy: 0.9921
Epoch 16/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0533 - accuracy: 0.9835 - val_loss: 0.0234 - val_accuracy: 0.9927
Epoch 17/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0515 - accuracy: 0.9836 - val_loss: 0.0250 - val_accuracy: 0.9916
Epoch 18/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0491 - accuracy: 0.9847 - val_loss: 0.0229 - val_accuracy: 0.9929
Epoch 19/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0490 - accuracy: 0.9847 - val_loss: 0.0245 - val_accuracy: 0.9917
Epoch 20/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0500 - accuracy: 0.9845 - val_loss: 0.0225 - val_accuracy: 0.9922
Epoch 21/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.0244 - val_accuracy: 0.9921
Epoch 22/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0460 - accuracy: 0.9857 - val_loss: 0.0217 - val_accuracy: 0.9931
Epoch 23/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.0220 - val_accuracy: 0.9923
Epoch 24/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0432 - accuracy: 0.9864 - val_loss: 0.0248 - val_accuracy: 0.9921
Epoch 25/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0433 - accuracy: 0.9863 - val_loss: 0.0234 - val_accuracy: 0.9917
Epoch 26/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0417 - accuracy: 0.9868 - val_loss: 0.0223 - val_accuracy: 0.9928
Epoch 27/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0415 - accuracy: 0.9872 - val_loss: 0.0223 - val_accuracy: 0.9925
Epoch 28/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0427 - accuracy: 0.9865 - val_loss: 0.0222 - val_accuracy: 0.9921
Epoch 29/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0432 - accuracy: 0.9868 - val_loss: 0.0240 - val_accuracy: 0.9915
Epoch 30/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0407 - accuracy: 0.9872 - val_loss: 0.0216 - val_accuracy: 0.9916
Epoch 31/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.0230 - val_accuracy: 0.9924
Epoch 32/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0405 - accuracy: 0.9871 - val_loss: 0.0231 - val_accuracy: 0.9925
Epoch 33/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.0233 - val_accuracy: 0.9926
Epoch 34/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0380 - accuracy: 0.9879 - val_loss: 0.0243 - val_accuracy: 0.9925
Epoch 35/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0352 - accuracy: 0.9888 - val_loss: 0.0219 - val_accuracy: 0.9927
Epoch 36/50
938/938 [==============================] - 7s 7ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.0232 - val_accuracy: 0.9917
Epoch 37/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0383 - accuracy: 0.9879 - val_loss: 0.0228 - val_accuracy: 0.9921
Epoch 38/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0352 - accuracy: 0.9890 - val_loss: 0.0222 - val_accuracy: 0.9924
Epoch 39/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0369 - accuracy: 0.9884 - val_loss: 0.0231 - val_accuracy: 0.9924
Epoch 40/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0370 - accuracy: 0.9884 - val_loss: 0.0221 - val_accuracy: 0.9927
Epoch 41/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0361 - accuracy: 0.9884 - val_loss: 0.0234 - val_accuracy: 0.9932
Epoch 42/50
938/938 [==============================] - 7s 7ms/step - loss: 0.0370 - accuracy: 0.9885 - val_loss: 0.0200 - val_accuracy: 0.9938
Epoch 43/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0344 - accuracy: 0.9890 - val_loss: 0.0210 - val_accuracy: 0.9935
Epoch 44/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0353 - accuracy: 0.9883 - val_loss: 0.0221 - val_accuracy: 0.9930
Epoch 45/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0336 - accuracy: 0.9889 - val_loss: 0.0223 - val_accuracy: 0.9927
Epoch 46/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0349 - accuracy: 0.9888 - val_loss: 0.0226 - val_accuracy: 0.9921
Epoch 47/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 0.0226 - val_accuracy: 0.9928
Epoch 48/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.0224 - val_accuracy: 0.9932
Epoch 49/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.0243 - val_accuracy: 0.9929
Epoch 50/50
938/938 [==============================] - 6s 7ms/step - loss: 0.0331 - accuracy: 0.9895 - val_loss: 0.0222 - val_accuracy: 0.9933
<keras.callbacks.History at 0x7f5f557d5890>
Testing
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
Test loss: 0.022164877504110336
Test accuracy: 0.9933000206947327
Prediction
import pylab as plt

plt.imshow(x_test[122].reshape(28,28),cmap='gray')
plt.show()

import numpy as np
li=[0.5, 0,0.4, 0,0,0]
li=np.array(li)
thresholded = (li>=0.5)*1
thresholded 
array([1, 0, 0, 0, 0, 0])
import numpy as np
prediction = model.predict(x_test[122:123])
print('Prediction Score:\n',prediction[0])
thresholded = (prediction>0.5)*1
print('\nThresholded Score:\n',thresholded[0])
print('\nPredicted Digit:\n',np.where(thresholded == 1)[1])
Prediction Score:
 [4.4959827e-17 1.3910924e-08 8.7462954e-12 9.3724632e-11 4.6323334e-10
 4.8262169e-14 2.4765285e-20 1.0000000e+00 9.9627287e-15 1.2862011e-09]

Thresholded Score:
 [0 0 0 0 0 0 0 1 0 0]

Predicted Digit:
 [7]
Part 2: Applications of Convolutional Neural Network
MobileNet Models
model = MobileNet(input_shape=None, alpha=0.25, depth_multiplier=1, dropout=1e-3, 
                                 include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)

model.summary()
Model: "mobilenet_0.25_224"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_3 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 conv1 (Conv2D)              (None, 112, 112, 8)       216       
                                                                 
 conv1_bn (BatchNormalizatio  (None, 112, 112, 8)      32        
 n)                                                              
                                                                 
 conv1_relu (ReLU)           (None, 112, 112, 8)       0         
                                                                 
 conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 8)      72        
                                                                 
 conv_dw_1_bn (BatchNormaliz  (None, 112, 112, 8)      32        
 ation)                                                          
                                                                 
 conv_dw_1_relu (ReLU)       (None, 112, 112, 8)       0         
                                                                 
 conv_pw_1 (Conv2D)          (None, 112, 112, 16)      128       
                                                                 
 conv_pw_1_bn (BatchNormaliz  (None, 112, 112, 16)     64        
 ation)                                                          
                                                                 
 conv_pw_1_relu (ReLU)       (None, 112, 112, 16)      0         
                                                                 
 conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 16)      0         
                                                                 
 conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 16)       144       
                                                                 
 conv_dw_2_bn (BatchNormaliz  (None, 56, 56, 16)       64        
 ation)                                                          
                                                                 
 conv_dw_2_relu (ReLU)       (None, 56, 56, 16)        0         
                                                                 
 conv_pw_2 (Conv2D)          (None, 56, 56, 32)        512       
                                                                 
 conv_pw_2_bn (BatchNormaliz  (None, 56, 56, 32)       128       
 ation)                                                          
                                                                 
 conv_pw_2_relu (ReLU)       (None, 56, 56, 32)        0         
                                                                 
 conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 32)       288       
                                                                 
 conv_dw_3_bn (BatchNormaliz  (None, 56, 56, 32)       128       
 ation)                                                          
                                                                 
 conv_dw_3_relu (ReLU)       (None, 56, 56, 32)        0         
                                                                 
 conv_pw_3 (Conv2D)          (None, 56, 56, 32)        1024      
                                                                 
 conv_pw_3_bn (BatchNormaliz  (None, 56, 56, 32)       128       
 ation)                                                          
                                                                 
 conv_pw_3_relu (ReLU)       (None, 56, 56, 32)        0         
                                                                 
 conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 32)        0         
                                                                 
 conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 32)       288       
                                                                 
 conv_dw_4_bn (BatchNormaliz  (None, 28, 28, 32)       128       
 ation)                                                          
                                                                 
 conv_dw_4_relu (ReLU)       (None, 28, 28, 32)        0         
                                                                 
 conv_pw_4 (Conv2D)          (None, 28, 28, 64)        2048      
                                                                 
 conv_pw_4_bn (BatchNormaliz  (None, 28, 28, 64)       256       
 ation)                                                          
                                                                 
 conv_pw_4_relu (ReLU)       (None, 28, 28, 64)        0         
                                                                 
 conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 64)       576       
                                                                 
 conv_dw_5_bn (BatchNormaliz  (None, 28, 28, 64)       256       
 ation)                                                          
                                                                 
 conv_dw_5_relu (ReLU)       (None, 28, 28, 64)        0         
                                                                 
 conv_pw_5 (Conv2D)          (None, 28, 28, 64)        4096      
                                                                 
 conv_pw_5_bn (BatchNormaliz  (None, 28, 28, 64)       256       
 ation)                                                          
                                                                 
 conv_pw_5_relu (ReLU)       (None, 28, 28, 64)        0         
                                                                 
 conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 64)        0         
                                                                 
 conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 64)       576       
                                                                 
 conv_dw_6_bn (BatchNormaliz  (None, 14, 14, 64)       256       
 ation)                                                          
                                                                 
 conv_dw_6_relu (ReLU)       (None, 14, 14, 64)        0         
                                                                 
 conv_pw_6 (Conv2D)          (None, 14, 14, 128)       8192      
                                                                 
 conv_pw_6_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_pw_6_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 128)      1152      
                                                                 
 conv_dw_7_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_dw_7_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_pw_7 (Conv2D)          (None, 14, 14, 128)       16384     
                                                                 
 conv_pw_7_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_pw_7_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 128)      1152      
                                                                 
 conv_dw_8_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_dw_8_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_pw_8 (Conv2D)          (None, 14, 14, 128)       16384     
                                                                 
 conv_pw_8_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_pw_8_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 128)      1152      
                                                                 
 conv_dw_9_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_dw_9_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_pw_9 (Conv2D)          (None, 14, 14, 128)       16384     
                                                                 
 conv_pw_9_bn (BatchNormaliz  (None, 14, 14, 128)      512       
 ation)                                                          
                                                                 
 conv_pw_9_relu (ReLU)       (None, 14, 14, 128)       0         
                                                                 
 conv_dw_10 (DepthwiseConv2D  (None, 14, 14, 128)      1152      
 )                                                               
                                                                 
 conv_dw_10_bn (BatchNormali  (None, 14, 14, 128)      512       
 zation)                                                         
                                                                 
 conv_dw_10_relu (ReLU)      (None, 14, 14, 128)       0         
                                                                 
 conv_pw_10 (Conv2D)         (None, 14, 14, 128)       16384     
                                                                 
 conv_pw_10_bn (BatchNormali  (None, 14, 14, 128)      512       
 zation)                                                         
                                                                 
 conv_pw_10_relu (ReLU)      (None, 14, 14, 128)       0         
                                                                 
 conv_dw_11 (DepthwiseConv2D  (None, 14, 14, 128)      1152      
 )                                                               
                                                                 
 conv_dw_11_bn (BatchNormali  (None, 14, 14, 128)      512       
 zation)                                                         
                                                                 
 conv_dw_11_relu (ReLU)      (None, 14, 14, 128)       0         
                                                                 
 conv_pw_11 (Conv2D)         (None, 14, 14, 128)       16384     
                                                                 
 conv_pw_11_bn (BatchNormali  (None, 14, 14, 128)      512       
 zation)                                                         
                                                                 
 conv_pw_11_relu (ReLU)      (None, 14, 14, 128)       0         
                                                                 
 conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 128)      0         
                                                                 
 conv_dw_12 (DepthwiseConv2D  (None, 7, 7, 128)        1152      
 )                                                               
                                                                 
 conv_dw_12_bn (BatchNormali  (None, 7, 7, 128)        512       
 zation)                                                         
                                                                 
 conv_dw_12_relu (ReLU)      (None, 7, 7, 128)         0         
                                                                 
 conv_pw_12 (Conv2D)         (None, 7, 7, 256)         32768     
                                                                 
 conv_pw_12_bn (BatchNormali  (None, 7, 7, 256)        1024      
 zation)                                                         
                                                                 
 conv_pw_12_relu (ReLU)      (None, 7, 7, 256)         0         
                                                                 
 conv_dw_13 (DepthwiseConv2D  (None, 7, 7, 256)        2304      
 )                                                               
                                                                 
 conv_dw_13_bn (BatchNormali  (None, 7, 7, 256)        1024      
 zation)                                                         
                                                                 
 conv_dw_13_relu (ReLU)      (None, 7, 7, 256)         0         
                                                                 
 conv_pw_13 (Conv2D)         (None, 7, 7, 256)         65536     
                                                                 
 conv_pw_13_bn (BatchNormali  (None, 7, 7, 256)        1024      
 zation)                                                         
                                                                 
 conv_pw_13_relu (ReLU)      (None, 7, 7, 256)         0         
                                                                 
 global_average_pooling2d_2   (None, 1, 1, 256)        0         
 (GlobalAveragePooling2D)                                        
                                                                 
 dropout (Dropout)           (None, 1, 1, 256)         0         
                                                                 
 conv_preds (Conv2D)         (None, 1, 1, 1000)        257000    
                                                                 
 reshape_2 (Reshape)         (None, 1000)              0         
                                                                 
 predictions (Activation)    (None, 1000)              0         
                                                                 
=================================================================
Total params: 475,544
Trainable params: 470,072
Non-trainable params: 5,472
_________________________________________________________________
Classify images
# Write the image name below

img_path = 'download.png'

img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

preds = model.predict(x)
print('Predicted:\n', decode_predictions(preds))
Predicted:
 [[('n04372370', 'switch', 0.08513701), ('n04376876', 'syringe', 0.08170768), ('n03937543', 'pill_bottle', 0.072473675), ('n04286575', 'spotlight', 0.06193055), ('n04548280', 'wall_clock', 0.060954913)]]
Extract CNN features
features = model.predict(x)
print('\nFeature Shape:\n',features.shape)
print('\nFeatures:\n',features)
Feature Shape:
 (1, 1000)

Features:
 [[8.45947000e-07 3.36881712e-05 3.74933043e-05 5.82506436e-05
  3.58010555e-04 1.30481646e-06 1.19297079e-06 5.43353963e-07
  7.97340022e-07 7.50879735e-07 8.03708957e-08 3.21906015e-07
  5.28705527e-07 5.26068288e-06 4.23359688e-06 5.13450118e-07
  1.13920839e-06 7.28586599e-07 1.18359048e-05 1.99680812e-06
  2.22620724e-06 8.47011324e-06 6.69349492e-06 1.69132927e-05
  2.88863475e-06 1.93076474e-07 9.55915584e-06 1.22065630e-05
  6.92152298e-06 9.09219398e-06 2.12008757e-07 2.30802721e-06
  5.01166369e-07 5.93039829e-07 1.14026693e-06 2.89345439e-06
  1.54929894e-05 9.54080264e-08 1.02650365e-05 3.32314187e-07
  1.11215127e-06 2.47342456e-07 1.20754251e-07 2.30538163e-07
  7.79244090e-07 8.50762802e-08 5.35873414e-06 1.64435073e-06
  1.74112049e-08 5.69942722e-06 2.72256566e-05 4.88349860e-06
  2.27028281e-07 9.77943955e-07 2.69297487e-07 2.27126293e-06
  4.70327450e-07 2.41511589e-09 1.23219452e-05 7.85076509e-07
  1.16172828e-06 5.53317125e-09 5.15772669e-08 5.35015772e-07
  4.40684380e-06 2.15316527e-06 5.01346619e-07 3.06644466e-07
  9.23817140e-07 6.36167670e-06 1.09727250e-06 3.18060047e-04
  4.03955710e-06 1.75128836e-04 6.13226148e-05 1.39724370e-05
  7.05872390e-06 1.06437083e-05 7.28389614e-06 5.05378703e-04
  1.81536925e-05 9.14083273e-07 1.03910156e-06 3.84841769e-07
  1.55231746e-07 5.34510889e-07 4.57049254e-07 3.94861945e-06
  4.55446326e-07 2.01702587e-05 5.77227688e-08 1.69310204e-06
  1.81588675e-07 6.02606974e-07 1.56649388e-04 1.36590003e-07
  2.95888572e-06 1.00044563e-05 1.82505715e-07 9.87835374e-05
  1.95326546e-04 5.80626249e-07 4.50143204e-07 2.11548886e-05
  2.38948792e-06 2.65378418e-07 3.23205836e-06 8.46647657e-04
  1.01561136e-05 8.94002596e-07 4.75618117e-06 6.25680623e-05
  4.65439552e-05 1.47742958e-05 4.48055944e-05 2.01187158e-05
  8.42825375e-06 1.00675015e-05 2.11765439e-07 8.97517111e-07
  1.75190189e-06 1.07815913e-06 5.33002151e-07 3.68026853e-08
  7.29274120e-07 2.02347155e-06 7.30546250e-04 1.05763493e-05
  3.91064987e-06 5.59236069e-05 7.47385184e-06 1.34533741e-06
  1.89767597e-04 1.41882211e-07 1.15637667e-05 2.16410035e-06
  4.37919334e-06 2.71908357e-05 2.83431888e-07 1.08891618e-06
  9.76961473e-07 1.56296153e-06 3.09327277e-07 3.88720764e-05
  1.94035601e-05 3.39077140e-08 1.45484807e-06 2.23808868e-07
  8.02926024e-06 1.82235340e-06 1.33929268e-06 4.01955504e-06
  3.68670112e-06 5.62646928e-05 2.90295538e-05 2.01347611e-05
  3.31648857e-06 4.26234010e-06 5.61153399e-07 5.42159853e-07
  9.82948677e-06 1.38385576e-06 8.09040955e-07 2.76611922e-06
  5.18953300e-07 1.82726819e-06 5.63279855e-07 1.50901514e-07
  1.77107245e-06 3.19880741e-07 2.36092683e-06 2.21008264e-07
  8.17427335e-08 2.81434382e-07 3.27906650e-06 8.63880268e-06
  6.89550802e-07 1.34914831e-06 5.44099930e-06 1.58932962e-05
  2.42263764e-06 1.97602117e-06 7.49993387e-06 4.35688780e-05
  1.25182423e-05 2.06603741e-04 6.19406710e-05 8.08312980e-05
  2.10278972e-06 5.20321664e-05 2.23360476e-05 2.39037240e-06
  3.69751615e-05 7.02028365e-06 2.64935406e-05 3.93492110e-06
  4.47594939e-05 5.06405377e-05 1.62560973e-05 2.62280664e-04
  8.56656188e-05 2.78749358e-05 1.10776009e-05 1.34498859e-03
  1.19963239e-04 1.87918577e-05 6.32400315e-06 1.25152710e-05
  2.10460439e-05 3.45359194e-06 1.32799494e-06 3.65102665e-06
  1.12421731e-06 2.79207097e-06 4.93980087e-06 2.18485661e-06
  5.05628395e-06 1.96118690e-05 1.49229606e-06 1.93260148e-05
  3.94387207e-05 1.25735503e-04 6.81104393e-06 3.76959448e-04
  7.90514387e-05 5.77367302e-07 1.16405436e-05 1.42207932e-06
  9.77273157e-06 6.56073234e-06 1.61847606e-06 1.55706721e-06
  6.71840171e-05 2.78417021e-04 4.34491130e-06 9.96203653e-07
  4.46260401e-07 1.05319566e-06 1.55014357e-06 4.31477156e-06
  2.71629847e-06 4.41559592e-07 4.38080588e-06 2.24074256e-06
  3.63486811e-06 8.79458184e-06 2.47239143e-07 1.20179993e-06
  4.37179870e-06 2.72306829e-06 1.75917137e-06 3.86327247e-06
  2.69384967e-04 1.57985221e-06 2.21974660e-05 2.23491071e-07
  9.81476187e-05 9.99931581e-06 8.73377285e-05 2.40094487e-05
  2.12968080e-05 2.36522214e-06 2.40881836e-05 2.96153985e-06
  4.30775754e-06 3.51044655e-05 1.92956304e-05 4.39998839e-05
  3.06631051e-07 6.00555722e-06 2.97688603e-05 2.58959631e-07
  2.49008326e-06 1.51280483e-05 1.49601090e-06 1.22418641e-07
  5.49961271e-07 1.51342317e-06 3.33226296e-07 4.67436112e-05
  7.01667886e-07 1.84226349e-06 1.84879161e-06 7.42902921e-05
  6.74418470e-06 3.50933010e-06 2.68642498e-06 2.41486396e-06
  6.91351090e-07 1.54496249e-07 1.15411808e-07 1.77470110e-06
  1.64330663e-06 3.17651825e-07 1.86819375e-06 5.46953515e-06
  2.67140167e-05 1.52541963e-06 4.20245186e-07 3.19249267e-07
  6.15657609e-06 2.91274614e-06 7.54033954e-06 5.66430344e-06
  4.63487004e-06 1.68347219e-06 1.16843212e-05 4.04393395e-05
  1.27907333e-05 6.48980358e-06 6.57364071e-05 8.81502092e-06
  4.22590674e-05 7.12703695e-05 7.01343160e-05 1.42411845e-05
  1.78294968e-07 2.79684828e-05 5.62411913e-07 2.94691199e-06
  1.72323616e-05 7.90708157e-07 1.68194362e-07 7.94202251e-08
  1.66922950e-06 6.99706320e-08 1.66555481e-06 2.91238357e-05
  1.58366947e-05 3.46326260e-06 1.82072498e-07 1.10906069e-06
  1.38525347e-05 2.51134887e-04 7.58286774e-07 1.27571161e-06
  7.38823616e-08 1.25727587e-04 1.13943934e-05 1.70439375e-06
  8.89621333e-06 6.60394608e-06 2.25964823e-06 4.63251081e-06
  1.46907735e-06 6.46511853e-06 8.64103549e-06 4.14453425e-05
  2.41966382e-06 6.33985110e-07 5.12674092e-07 2.16152316e-07
  7.25996642e-07 3.07625010e-06 1.32060595e-05 2.04910812e-05
  9.06176683e-06 3.90263176e-06 2.10147255e-06 9.97940901e-07
  1.94362951e-06 5.43535461e-05 5.24262805e-06 1.70737087e-06
  2.86543400e-06 1.00260713e-05 1.07905353e-05 3.65162191e-06
  1.35424725e-05 3.50819892e-06 1.57057073e-07 6.65378366e-07
  2.22908275e-05 5.22676282e-06 1.77724069e-05 9.77064815e-07
  1.44100932e-06 4.65245148e-06 3.23503127e-06 8.05616764e-06
  4.75377328e-06 4.69029146e-06 2.91880247e-06 9.93913886e-07
  2.31659874e-06 2.52680297e-06 2.00206705e-06 1.36115682e-06
  1.69810028e-06 4.80994402e-07 3.36415184e-07 1.13535486e-06
  5.78484503e-07 5.80226015e-07 1.55484872e-06 3.40105771e-06
  3.96755377e-06 2.24312953e-05 8.00207781e-04 3.35775917e-06
  8.50009474e-06 1.00360124e-03 2.89851159e-04 9.75496660e-04
  2.33559334e-03 1.13029089e-02 1.59249088e-04 7.61377742e-04
  1.35103916e-03 4.70714122e-02 1.35871102e-04 2.91054130e-05
  1.48482202e-03 6.27002819e-03 1.70928048e-04 1.05427804e-04
  2.94964921e-05 1.41945691e-03 1.99006405e-03 3.42080632e-04
  1.05619307e-04 7.50912732e-05 9.62889826e-05 3.48611095e-04
  2.83510017e-04 8.66368704e-04 9.38643178e-04 1.15218596e-03
  6.58228528e-05 1.15200346e-05 1.02681838e-06 4.91652718e-05
  2.54408835e-04 3.11103249e-05 2.29872512e-05 7.75906723e-04
  3.05615831e-05 1.25589781e-03 1.76320167e-03 1.83763204e-05
  7.00783930e-05 1.83420387e-04 1.28027075e-03 2.28833560e-05
  7.14018006e-06 3.37186566e-06 1.31347682e-03 1.46973194e-04
  6.75758638e-04 1.42295845e-03 1.22199690e-05 1.56053029e-05
  2.69084035e-06 6.30017748e-05 2.40731661e-05 1.65179151e-03
  2.63433438e-04 1.68520943e-04 5.12340057e-06 6.67712811e-06
  1.86356818e-04 2.68183037e-04 2.89826683e-04 9.71434114e-04
  6.69860048e-04 3.59710946e-04 1.37357521e-04 3.95383495e-06
  2.45494972e-04 3.15012177e-04 1.46838017e-02 3.23645538e-04
  1.46639213e-04 8.33133352e-04 4.39164796e-06 1.03671809e-04
  4.90490811e-06 2.84129841e-04 1.35632884e-03 5.44688191e-05
  1.91220059e-03 4.65833675e-03 4.03204991e-04 2.74171005e-04
  2.70196833e-05 1.60142477e-03 4.53510525e-04 1.47363942e-04
  9.86179220e-04 1.63606514e-04 4.11931287e-05 8.58628628e-05
  1.63875186e-04 1.81188079e-04 1.51491922e-03 6.70132576e-04
  1.73606022e-04 1.44611706e-03 1.64678262e-03 9.78552387e-04
  9.72761598e-04 2.44128478e-06 7.42291331e-06 1.04941995e-04
  2.90660537e-04 7.99433546e-05 7.47431477e-04 1.02571317e-03
  3.84363532e-03 5.71070541e-06 1.43090985e-03 4.30848268e-05
  3.19646858e-03 7.80303453e-05 2.92872892e-06 5.62911519e-05
  1.07815256e-04 8.69517401e-03 1.11379915e-04 6.96172006e-04
  2.17246816e-05 1.54405745e-04 2.20749207e-05 2.14722560e-04
  6.32769486e-04 2.85477308e-05 8.35447048e-04 1.69625948e-03
  2.02670763e-03 1.38915144e-04 3.36782970e-02 5.74996043e-03
  1.06873085e-04 8.02351133e-06 1.00776837e-04 4.20934703e-05
  7.94985099e-04 1.92644734e-06 4.21401666e-04 1.18664548e-05
  4.73044493e-04 6.21284533e-04 7.57301517e-04 7.27039529e-04
  2.68414478e-05 3.43532854e-04 3.41777061e-03 4.75443776e-05
  1.10171306e-04 6.47334266e-04 7.10120425e-04 8.77217171e-05
  8.66321148e-04 6.69175899e-03 2.30688718e-04 3.64339008e-04
  8.64632384e-05 1.77013897e-03 4.24243248e-04 6.43160078e-04
  6.48037894e-05 8.53777572e-04 8.14510859e-04 2.48653290e-04
  3.65494052e-05 2.02910873e-04 4.98617283e-07 1.06929212e-04
  1.15476114e-05 4.19819233e-04 3.43144493e-05 1.77531037e-04
  8.28751421e-04 4.50993156e-07 2.32293809e-04 6.09329654e-05
  2.05184870e-06 6.57791752e-05 8.96018355e-06 2.51448131e-03
  9.87494332e-06 5.76281964e-05 1.10986957e-05 3.01087653e-04
  3.45128588e-04 5.54259692e-04 1.49316751e-04 5.71708719e-04
  2.91336299e-04 3.87569726e-03 9.51649563e-05 1.53375426e-04
  7.00852717e-04 1.41138688e-03 7.03777914e-05 5.29863755e-04
  8.80666834e-04 6.05327114e-06 5.63375303e-04 7.72631261e-04
  4.54189815e-03 1.73759224e-06 2.82307774e-05 3.57640283e-06
  1.68431696e-04 9.15971294e-04 1.53000921e-03 2.26540398e-03
  3.89080287e-06 1.28300989e-03 8.30677072e-06 1.48018764e-03
  3.68693463e-07 9.80849727e-05 4.95871427e-06 1.18677226e-05
  1.10812252e-04 3.66476379e-05 1.26111569e-04 1.00514377e-02
  5.74115256e-04 8.22313086e-05 1.04552112e-03 3.81776685e-04
  4.34835638e-05 1.05611151e-04 2.77081877e-03 4.31945518e-05
  7.16499635e-04 2.01704257e-04 3.62623098e-07 3.96270625e-04
  2.41661735e-04 3.62504565e-04 1.12693269e-05 1.13030539e-04
  1.83849799e-04 7.06922077e-03 7.53926861e-06 8.60305136e-07
  3.95509269e-05 1.78257578e-05 9.86221130e-04 9.24368069e-05
  4.74039167e-02 2.83524219e-06 1.65278802e-03 6.63638522e-04
  9.85347782e-04 4.28233725e-05 4.69727721e-03 8.37631378e-05
  4.00791287e-05 8.83672765e-05 1.61865610e-04 7.14136477e-05
  2.03127987e-04 4.90297424e-03 2.21253003e-06 1.80327697e-04
  3.76692973e-04 1.29532430e-06 1.59510819e-03 1.11646114e-04
  1.17824669e-03 1.27104184e-04 5.03687945e-04 5.58105203e-05
  1.82867167e-04 1.50340973e-04 1.85178069e-04 1.36542449e-05
  1.28257602e-06 1.53099565e-04 3.80784762e-03 3.77200740e-05
  6.70661902e-05 4.64408007e-03 3.34781034e-05 1.78216025e-04
  6.45906766e-05 4.63255215e-04 1.09918343e-04 6.07239257e-04
  8.43500038e-06 1.28365049e-04 1.20054574e-04 3.01711843e-04
  2.29632854e-03 5.76979858e-07 1.04073899e-06 3.39053586e-05
  7.65524979e-04 3.60617232e-05 3.05084366e-04 1.84281176e-04
  9.70372988e-04 1.37272514e-06 2.27067620e-04 4.65124054e-03
  1.01773196e-03 8.80896929e-04 9.94788206e-05 1.69877359e-03
  1.31585577e-04 4.83172516e-05 2.04909811e-05 3.67467175e-04
  1.04478408e-04 1.48952080e-04 1.55998720e-02 2.07373896e-03
  2.18922825e-04 3.60955810e-03 1.32476926e-04 8.59465445e-07
  1.99791021e-03 6.05894638e-05 1.06649248e-04 5.37043496e-04
  7.24736750e-02 3.18935090e-05 5.38759159e-05 1.95449884e-05
  1.52703619e-03 3.73494113e-05 4.74363378e-06 4.84846714e-05
  1.30722037e-04 9.28725640e-04 2.18347544e-04 7.50160485e-04
  3.05864553e-04 5.96534926e-04 1.37033552e-04 1.94283643e-06
  2.81321292e-04 7.96258028e-05 4.27691702e-06 1.54730697e-05
  3.06204287e-03 7.88674424e-06 4.33097640e-03 3.35612218e-04
  5.68929175e-03 1.12746423e-02 7.20051321e-05 2.22927192e-04
  1.35459020e-04 1.11831643e-03 1.22145639e-05 2.47983462e-05
  3.94489916e-07 1.58571661e-03 9.91826528e-04 6.36954443e-04
  3.80410347e-04 7.66989018e-04 4.77663634e-05 1.31801731e-04
  1.35619880e-03 2.80879787e-04 1.09286331e-04 2.77859345e-03
  2.62344605e-03 6.26669556e-04 2.66337331e-04 8.01709853e-03
  5.03393539e-06 3.34133441e-03 7.49651178e-07 5.88903436e-04
  7.63472985e-04 2.96798506e-04 2.74934519e-06 1.89640673e-06
  5.42567468e-05 1.73488181e-04 2.70774169e-03 3.44105138e-05
  8.94427358e-05 2.45227071e-04 2.98076239e-03 2.01118607e-02
  9.17336065e-03 6.23957612e-06 4.87060612e-03 1.53981062e-04
  4.51766973e-05 8.40443536e-05 2.33199753e-04 7.95627595e-04
  1.02542550e-03 1.15944986e-05 3.03415139e-03 1.45429385e-05
  2.52655609e-05 7.55857172e-07 8.75048514e-04 7.33991183e-05
  3.81723366e-04 4.29177599e-05 3.07211048e-05 6.09597890e-04
  1.13528338e-03 2.45727006e-05 2.80648169e-06 2.47274875e-04
  2.66114557e-06 1.42945180e-04 1.48155948e-03 8.93838611e-03
  1.82285008e-03 8.18261236e-04 3.55865996e-05 7.71819672e-04
  1.31793240e-05 1.08980912e-05 6.19305484e-02 7.26977421e-04
  1.42787867e-05 4.17787078e-06 7.97790744e-06 1.10733445e-05
  6.82234895e-06 2.19847461e-05 2.46240990e-03 5.62847359e-03
  5.15821390e-04 3.33229473e-05 6.30169758e-04 7.49842147e-05
  1.24214188e-04 1.07598622e-04 5.55780889e-06 7.30909014e-05
  1.71958920e-04 1.08608074e-04 1.35371854e-04 1.44243997e-04
  2.58315890e-03 5.55533416e-06 5.65767914e-06 1.02975417e-03
  8.51370096e-02 8.17076787e-02 8.49419623e-04 1.65298581e-04
  1.00176898e-03 3.52678471e-05 6.06817812e-05 3.42381396e-03
  7.48765597e-05 9.88183001e-06 8.93114367e-04 2.27443103e-04
  2.01516668e-05 4.07963307e-05 8.09171324e-05 1.31325633e-03
  1.07768064e-05 5.64736547e-03 1.94650842e-04 3.96105861e-06
  1.03850376e-04 4.95111544e-06 4.83833806e-04 4.64148732e-04
  1.18179049e-03 3.40491965e-06 2.30524329e-05 1.25107845e-05
  4.92335204e-03 2.49539647e-04 1.72709424e-05 6.40973121e-06
  7.14841823e-04 1.20948993e-04 3.32228979e-03 1.62823897e-04
  1.79459621e-05 6.31401828e-03 7.71091378e-04 7.04766571e-05
  1.14425457e-04 1.33747506e-06 1.52116554e-04 5.42529676e-07
  2.34695362e-05 1.28341548e-04 2.92707155e-05 2.85914890e-03
  6.09549135e-02 4.17754418e-05 9.91359004e-04 1.46358053e-03
  7.13993935e-03 8.42033623e-05 1.96053996e-03 8.43367670e-05
  7.32551096e-03 2.43424984e-06 5.62905334e-04 4.70223196e-04
  4.32851695e-04 2.17429306e-02 6.76944126e-07 7.98989204e-04
  1.38337084e-03 2.83758622e-04 1.31766838e-05 2.09243663e-05
  3.89002562e-05 7.32698070e-04 6.34163780e-06 6.65493935e-05
  3.67280591e-04 3.57492991e-06 8.43615489e-05 1.34091265e-03
  4.17396799e-03 7.89247046e-04 9.29916278e-05 8.73203608e-05
  5.13164468e-06 5.86579517e-05 7.94861194e-07 5.00887836e-05
  8.58283875e-06 7.55417350e-05 3.60762897e-07 2.93338453e-05
  3.49065340e-05 4.02276055e-06 8.06629214e-06 5.58474712e-05
  2.66023699e-06 6.43547901e-05 8.64845606e-06 5.57708290e-06
  1.75866307e-06 6.52294716e-07 2.27449709e-06 2.99373733e-05
  3.48605067e-06 1.95342068e-06 1.64663561e-06 9.49062989e-04
  6.89209992e-05 3.89372053e-06 7.19486707e-05 4.17547672e-05
  9.70537499e-08 5.71005194e-06 9.33096599e-05 4.25820417e-06
  2.51530832e-06 2.76413857e-06 1.91212661e-04 5.91515663e-06
  3.91783920e-04 1.19400593e-05 1.65029305e-05 2.54720760e-07
  9.91181696e-06 5.45070861e-07 2.27316050e-03 1.17724871e-04
  6.09131239e-04 2.34869985e-05 8.48255422e-06 1.26505882e-04
  2.15696098e-04 1.40152554e-06 3.27244561e-05 5.08419413e-04
  4.61443487e-05 4.92837862e-05 5.36632142e-04 1.99499232e-06
  2.10390790e-05 8.65388392e-06 4.54807650e-06 1.37601499e-04
  6.68021748e-05 1.58579296e-05 3.66697517e-09 1.73841618e-04
  5.27426380e-07 2.45140768e-06 1.01630794e-05 1.84082441e-04
  4.39730393e-05 7.77920150e-07 6.01765396e-06 6.95877952e-06
  6.89069975e-06 1.95917823e-06 4.82110227e-05 5.18817781e-03]]
Extract features from an arbitrary intermediate layer
model_minimal = Model(input=model.input, output=model.get_layer('conv_dw_2_relu').output)

conv_dw_2_relu_features = model_minimal.predict(x)
print('Features of conv_dw_2_relu:',conv_dw_2_relu_features.shape)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-37-394c08beca9c> in <module>()
----> 1 model_minimal = Model(input=model.input, output=model.get_layer('conv_dw_2_relu').output)
      2 
      3 conv_dw_2_relu_features = model_minimal.predict(x)
      4 print('Features of conv_dw_2_relu:',conv_dw_2_relu_features.shape)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    627     self._self_setattr_tracking = False  # pylint: disable=protected-access
    628     try:
--> 629       result = method(self, *args, **kwargs)
    630     finally:
    631       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)
---> 67       raise e.with_traceback(filtered_tb) from None
     68     finally:
     69       del filtered_tb

/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py in validate_kwargs(kwargs, allowed_kwargs, error_message)
   1172   for kwarg in kwargs:
   1173     if kwarg not in allowed_kwargs:
-> 1174       raise TypeError(error_message, kwarg)
   1175 
   1176 

TypeError: ('Keyword argument not understood:', 'input')
You can extract these features and use the base network as a feature extractor for your problems.
Part 3: Deep Convolution Layer Visualization
import matplotlib as mp
%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.contrib.slim as slim
from tensorflow.examples.tutorials.mnist import input_data
import math
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-38-03f63042c631> in <module>()
      3 import matplotlib.pyplot as plt
      4 import tensorflow as tf
----> 5 import tensorflow.contrib.slim as slim
      6 from tensorflow.examples.tutorials.mnist import input_data
      7 import math

ModuleNotFoundError: No module named 'tensorflow.contrib'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
"Open Examples" button below.
---------------------------------------------------------------------------
Extract Data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)
Model Building
tf.reset_default_graph()

x = tf.placeholder(tf.float32, [None, 784],name="x-in")
true_y = tf.placeholder(tf.float32, [None, 10],name="y-in")
keep_prob = tf.placeholder("float")

x_image = tf.reshape(x,[-1,28,28,1])
hidden_1 = slim.conv2d(x_image,5,[5,5])
pool_1 = slim.max_pool2d(hidden_1,[2,2])
hidden_2 = slim.conv2d(pool_1,5,[5,5])
pool_2 = slim.max_pool2d(hidden_2,[2,2])
hidden_3 = slim.conv2d(pool_2,20,[5,5])
hidden_3 = slim.dropout(hidden_3,keep_prob)
out_y = slim.fully_connected(slim.flatten(hidden_3),10,activation_fn=tf.nn.softmax)

cross_entropy = -tf.reduce_sum(true_y*tf.log(out_y))
correct_prediction = tf.equal(tf.argmax(out_y,1), tf.argmax(true_y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
Training
batchSize = 50
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)
for i in range(1001):
    batch = mnist.train.next_batch(batchSize)
    sess.run(train_step, feed_dict={x:batch[0],true_y:batch[1], keep_prob:0.5})
    if i % 100 == 0 and i != 0:
        trainAccuracy = sess.run(accuracy, feed_dict={x:batch[0],true_y:batch[1], keep_prob:1.0})
        print("step %d, training accuracy %g"%(i, trainAccuracy))
Testing accuracy
testAccuracy = sess.run(accuracy, feed_dict={x:mnist.test.images,true_y:mnist.test.labels, keep_prob:1.0})
print("test accuracy %g"%(testAccuracy))
Get activation values and plotting
def getActivations(layer,stimuli):
    units = sess.run(layer,feed_dict={x:np.reshape(stimuli,[1,784],order='F'),keep_prob:1.0})
    plotNNFilter(units)
    
def plotNNFilter(units):
    filters = units.shape[3]
    plt.figure(1, figsize=(20,20))
    n_columns = 6
    n_rows = math.ceil(filters / n_columns) + 1
    for i in range(filters):
        plt.subplot(n_rows, n_columns, i+1)
        plt.title('Filter ' + str(i))
        plt.imshow(units[0,:,:,i], interpolation="nearest", cmap="gray")
Input Image
imageToUse = mnist.test.images[0]
plt.imshow(np.reshape(imageToUse,[28,28]), interpolation="nearest", cmap="gray")
Activation in Layer 1
getActivations(hidden_1,imageToUse)
Activation in Layer 2
getActivations(hidden_2,imageToUse)
Activation in Layer 3
getActivations(hidden_3,imageToUse)
Part 4: Design Choices in Convolutional Neural Networks
Influence of convolution size
Model with (3 x 3) Convolution
K.clear_session()
start = timeit.default_timer()   
model = Sequential()
model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(Conv2D(16, (3, 3), activation='relu'))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
end = timeit.default_timer()
print("Time Taken to run the model:",end - start, "seconds")  
Model with (7 x 7) Convolution
# Write your code here 

# Use the same model design from the above cell 
Striding
Model with (7 x 7) Convolution with 2 Steps
start = timeit.default_timer()   
model = Sequential()
model.add(Conv2D(8, kernel_size=(7, 7), strides=2, activation='relu', input_shape=input_shape))
model.add(Conv2D(16, (7, 7), strides=2, activation='relu'))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
end = timeit.default_timer()
print("Time Taken to run the model:",end - start, "seconds")  
Padding
Model with (7 x 7) Convolution with Same Padding
start = timeit.default_timer()   
model = Sequential()
model.add(Conv2D(8, kernel_size=(7, 7), strides=1, padding='same', activation='relu', input_shape=input_shape))
model.add(Conv2D(16, (7, 7), strides=1, padding='same', activation='relu'))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
end = timeit.default_timer()
print("Time Taken to run the model:",end - start, "seconds")  
Pooling
Model with (3 x 3) Convolution with Pooling (2 x 2)
start = timeit.default_timer()   
model = Sequential()
model.add(Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(16, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
end = timeit.default_timer()
print("Time Taken to run the model:",end - start, "seconds")  
Model with (3 x 3) Convolution with Pooling (3 x 3)
# Write your code here 

# Use the same model design from the above cell 
What are your findings?
